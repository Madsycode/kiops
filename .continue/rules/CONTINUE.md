# CONTINUE.md

This project is a **Generative AI-powered MLOps Platform** (specifically themed for 6G/Network applications). It allows a user to describe a Machine Learning application in natural language, after which the system automatically designs the architecture, writes the code, builds Docker containers, and simulates the training and deployment lifecycle.

Here is a breakdown of the components and the workflow:

### 1. File Breakdown

*   **`service.py` (The Frontend/Orchestrator)**
    *   **Role:** The main Streamlit application entry point.
    *   **Functionality:**
        *   Provides the UI for API configuration (Gemini or Ollama).
        *   **Tab 1:** Takes user intent (e.g., "Create a Network Load Predictor") and uses AI to generate a structured "RichMLAppProfile" (BOM), visualising it with Graphviz.
        *   **Tab 2:** Orchestrates the "Synthesis" phase. It prompts the AI to write Python training scripts and Dockerfiles, then instructs the Docker engine to build images and run containers.
        *   **Tab 3:** A chat interface to ask questions about the generated application profile.

*   **`generative.py` (The AI Brain)**
    *   **Role:** Interface for Large Language Models (LLMs).
    *   **Functionality:**
        *   Connects to either Google Gemini or a local Ollama instance.
        *   `query_ai_json`: Enforces structured JSON output based on the Pydantic schema in `models.py`.
        *   `query_ai_text`: Generates raw code (Python scripts, Dockerfiles) or chat responses.

*   **`models.py` (The Data Schema)**
    *   **Role:** Defines the data structures using Pydantic.
    *   **Functionality:**
        *   Defines the blueprint for an ML Application (`RichMLAppProfile`), including resource requirements (`ResourceConfig`), training/inference logic, and hooks for data observation and actions.
        *   This ensures the AI doesn't just hallucinate random text but produces a valid configuration object.

*   **`containerize.py` (The Infrastructure Layer)**
    *   **Role:** A wrapper around the Python Docker SDK.
    *   **Functionality:**
        *   **`build_custom_image`:** Accepts a Dockerfile string (generated by AI) and builds a real Docker image.
        *   **`run_container`:** Runs the generated Python scripts inside containers, managing volume mounts (`/context` and `/data`) so files persist between the host and container.
        *   **`copy_model`:** Utility to move artifacts (like `.pkl` files) between containers.

*   **`styles.py`**
    *   **Role:** UI Customization.
    *   **Functionality:** Injects CSS to give the Streamlit app a dark mode, "Cyberpunk/Tech" aesthetic.

### 2. The Application Workflow

1.  **Intent Definition:** The user types a requirement (e.g., "I need a model to optimize antenna tilt based on traffic").
2.  **Architecting:** The AI converts this text into a JSON object defining inputs, outputs, memory requirements, and hardware needs (GPU/CPU).
3.  **Code Generation:**
    *   The AI writes `train.py` (generating synthetic data and training a dummy model).
    *   The AI writes a `Dockerfile` to support that script.
4.  **Build & Train:** The `DockerExecutionEngine` builds the image and runs the training container. The user sees live logs in the UI.
5.  **Deployment Simulation:** The AI writes `serve.py`, and the system spins up a second container to simulate serving the model for inference.

### 3. Key Technologies Used
*   **Frontend:** Streamlit
*   **AI/LLM:** OpenAI API format (for Ollama) and Google Generative AI (Gemini).
*   **Validation:** Pydantic.
*   **Infrastructure:** Docker SDK for Python.
*   **Visualization:** Graphviz.

In summary, this is a **"Text-to-App"** prototype for MLOps, automating the transition from a vague idea to a running Docker container.